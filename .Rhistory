install.packages("baseballr")
install.packages("rtools")
library(baseballr)
library(dplyr)
bref_standings_on_date("2015-08-01", "NL East", from = FALSE)
# Sample data frame
mlb_data <- data.frame(
player_name = c("Player A", "Player B", "Player A", "Player C", "Player B"),
year = c(2022, 2022, 2023, 2023, 2023),
WAR = c(4.5, 3.7, 5.2, 2.9, 4.1)
)
# Rank players by yearly WAR
ranked_players <- mlb_data %>%
group_by(year) %>%
arrange(desc(WAR)) %>%
mutate(rank = row_number()) %>%
ungroup()
# Sample data frame
mlb_data <- data.frame(
player_name = c("Player A", "Player B", "Player A", "Player C", "Player B"),
year = c(2022, 2022, 2023, 2023, 2023),
WAR = c(4.5, 3.7, 5.2, 2.9, 4.1)
)
> mlb_data <- data.frame(
# Load necessary libraries
library(dplyr)
library(rbaseball)
install.packages("baseballr")
# You can install using the pacman package using the following code:
if (!requireNamespace('pacman', quietly = TRUE)){
install.packages('pacman')
}
pacman::p_load_current_gh("BillPetti/baseballr")
# Alternatively, using the devtools package:
if (!requireNamespace('devtools', quietly = TRUE)){
install.packages('devtools')
}
devtools::install_github(repo = "BillPetti/baseballr")
# install.packages("devtools")
devtools::install_github("BillPetti/baseballr", ref = "development_branch")
# Install rbaseball if you haven't already
install.packages("devtools")
devtools::install_github("BillPetti/rbaseball")
install.packages("devtools")
# Install rbaseball if you haven't already
install.packages("devtools")
devtools::install_github("BillPetti/rbaseball")
# Load necessary libraries
library(dplyr)
library(rbaseball)
# Install the baseballr package if you haven't already
install.packages("devtools")
devtools::install_github("BillPetti/baseballr")
# Load the package
library(baseballr)
library(dplyr)
# Set the year for which you want to pull the data
year <- 2023
# Get player stats for the specified year
# Note: Adjust the function based on your needs; this is an example
player_stats <- baseballr::scrape_statcast_savant(year = year)
# Select relevant columns (e.g., player name, team, and WAR)
war_data <- player_stats %>%
select(player_name, team, WAR) %>%
filter(!is.na(WAR))  # Remove entries without WAR
# Set the year for which you want to pull the data
year <- 2023
# Get player stats for the specified year
# Note: Adjust the function based on your needs; this is an example
player_stats <- baseballr::scrape_statcast_savant(year = year)
# Select relevant columns (e.g., player name, team, and WAR)
war_data <- player_stats %>%
select(player_name, team, WAR) %>%
filter(!is.na(WAR))  # Remove entries without WAR
#install package
install.packages("readxl")
library(readxl)
#import data
Boston<-read_excel("Prepare for lab 5_data.xlsx")
install.packages("rmarkdown")
install.packages("rtools")
install.packages("rmarkdown")
install.packages("rtools")
install.packages("stringi")
#ANOVA
oneway_test <- oneway.test(Overall_Average ~ Season,
data = Lab_5_assignment_data_1_1_,
var.equal = FALSE)
##change variable format: character to numeric
Lab_5_assignment_data_1_1_$Overall_Average<-as.numeric(Lab_5_assignment_data_1_1_$Overall_Average)
library(readxl)
Lab_5_Take_Home_Assignment <- read_excel("C:/Users/cejordan/Downloads/Lab 5 Take Home Assignment.xlsx")
View(Lab_5_Take_Home_Assignment)
HomeGame_Average<-subset(Boston, Season==2022)$HomeGame_Average
HomeGame_Average<-subset( Season==2022)$HomeGame_Average
HomeGame_Average<-subset(Season==2022)$HomeGame_Average
install.packages("readxl")
library(readxl)
Boston<-read_excel("Lab_5_Take_Home_Assignment.xlsx")
install.packages("readxl")
library(readxl)
#import data
Boston<-read_excel("Lab_5_Take_Home_Assignment.xlsx")
setwd("~/")
library(readxl)
Lab_5_Take_Home_Assignment <- read_excel("C:/Users/cejordan/Downloads/Lab 5 Take Home Assignment.xlsx")
View(Lab_5_Take_Home_Assignment)
HomeGame_Average<-subset(Season==2022)$HomeGame_Average
HomeGame_Average<-subset(2022)$HomeGame_Average
load("C:/Users/cejordan/Downloads/Take home Lab 5.RData")
#T-test
##Create subsets of data
attendance_21<-subset(Lab_5_assignment_data_1, Season==2021)$Overall_Average
attendance_22<-subset(Lab_5_assignment_data_1, Season==2022)$Overall_Average
#Test the difference of salaries between two years
attendance_21_22<-t.test(attendance_21,attendance_22, alternative="two.sided",var.equal=TRUE)
##view the t-test result
print(attendance_21_22)
#ANOVA
##Examining the difference of salaries among three seasons.
oneway_test <-oneway.test(Overall_Average ~ Season,
data = Lab_5_assignment_data_1,
var.equal = FALSE)
##view ANOVA result
oneway_test
#save the result
save.image(file="Take home Lab 5.RData")
library(readxl)
Lab_6_assignment_data <- read_excel("C:/Users/cejordan/Downloads/Lab 6 assignment_data.xlsx",
sheet = "Olympic Men's Basketball_24")
View(Lab_6_assignment_data)
```{r}
library(readxl)
nba<-read_excel("Lab_6_assignment_data")
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
plot(pressure)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
############################################################
# STAT Homework — Problems 1–3 (R script, no prompts)
# Problem 2 uses your actual column names from food_expenditure.csv
############################################################
options(stringsAsFactors = FALSE)
############################################################
# Problem 1 — Abalone Data
# Source: https://archive.ics.uci.edu/dataset/1/abalone
# Files needed: abalone.data  (rectangular data)
############################################################
# ---- 1A. Import and name columns ----
abalone <- read.csv("abalone.data", header = FALSE)
colnames(abalone) <- c(
"Sex","Length","Diameter","Height",
"WholeWeight","ShuckedWeight","VisceraWeight","ShellWeight","Rings"
)
# Quick checks
cat("\n[Problem 1] Abalone rows/cols:", nrow(abalone), "/", ncol(abalone), "\n")
cat("[Problem 1] Column names:\n")
print(colnames(abalone))
# ---- 1B. Count observations per sex ----
cat("\n[Problem 1] Counts by Sex:\n")
print(table(abalone$Sex))
# ---- 1C. Which weight has the highest correlation with Rings? ----
weights <- c("WholeWeight","ShuckedWeight","VisceraWeight","ShellWeight")
safe_cor <- function(x, y) {
ok <- is.finite(x) & is.finite(y)
if (sum(ok) < 3) return(NA_real_)
cor(x[ok], y[ok])
}
cors_all <- sapply(weights, function(w) safe_cor(abalone[[w]], abalone$Rings))
cat("\n[Problem 1] Correlations of weights with Rings (all sexes):\n")
print(round(cors_all, 4))
# Identify the weight with the largest absolute correlation
best_weight <- names(which.max(abs(cors_all)))
cat("\n[Problem 1] Weight with highest |correlation| with Rings:", best_weight, "\n")
# ---- 1D. For that weight, which sex has the highest correlation? ----
cors_by_sex <- sapply(split(abalone, abalone$Sex), function(df) {
safe_cor(df[[best_weight]], df$Rings)
})
cat("\n[Problem 1] Correlation between", best_weight, "and Rings by Sex:\n")
print(round(cors_by_sex, 4))
best_sex <- names(which.max(abs(cors_by_sex)))
cat("\n[Problem 1] Sex with highest |correlation| for", best_weight, ":", best_sex, "\n")
# ---- 1E. Weights of abalone(s) with the most rings ----
max_rings <- max(abalone$Rings, na.rm = TRUE)
heaviest_rows <- abalone[abalone$Rings == max_rings, c(weights, "Rings")]
cat("\n[Problem 1] Weights for abalone(s) with the most Rings (max =", max_rings, "):\n")
print(heaviest_rows)
# ---- 1F. % with viscera weight larger than shell weight ----
pct_viscera_gt_shell <- mean(abalone$VisceraWeight > abalone$ShellWeight, na.rm = TRUE) * 100
cat("\n[Problem 1] % with VisceraWeight > ShellWeight:", round(pct_viscera_gt_shell, 2), "%\n")
# ---- 1G. Correlation table: rows = Sex, cols = weights ----
corr_table <- do.call(rbind, lapply(split(abalone, abalone$Sex), function(df) {
sapply(weights, function(w) safe_cor(df[[w]], df$Rings))
}))
corr_table <- corr_table[order(rownames(corr_table)), , drop = FALSE]
cat("\n[Problem 1] Correlation table (rows = Sex, cols = weights; values = cor(weight, Rings)):\n")
print(round(corr_table, 4))
# ---- 1H. T-tests for Rings across sexes ----
cat("\n[Problem 1] Pairwise Welch t-tests for Rings by Sex (Bonferroni-adjusted p-values):\n")
print(pairwise.t.test(abalone$Rings, abalone$Sex, p.adjust.method = "bonferroni", pool.sd = FALSE))
cat("\n[Problem 1] Interpretation guidance:\n")
cat("- Look for pairs with adjusted p < 0.05 to conclude a significant difference in mean Rings between those sexes.\n")
############################################################
# Problem 2 — Food Expenditure Data
# Uses your actual column names from the uploaded CSV
# File needed: food_expenditure.csv (same directory or provide full path)
############################################################
# ---- 2A. Import ----
food <- read.csv("food_expenditure.csv", header = TRUE, check.names = FALSE)
# ---- 2B. Rename variables for easier use (your exact names) ----
names(food)[names(food) == "What is your age?"] <- "Age"
names(food)[names(food) == "What state do you live in?"] <- "State"
names(food)[names(food) == "What currency are you reporting your food expenditures in?"] <- "Currency"
names(food)[names(food) == "What was your total food expenditure in the last week?"] <- "FoodTotal"
names(food)[names(food) == "What was your total food expenditures at grocery stores in the last week?"] <- "FoodGrocery"
names(food)[names(food) == "What was your food expenditure while dining out in the last week?"] <- "FoodDining"
names(food)[names(food) == "What was your food expenditure (miscellaneous) in the last week?"] <- "FoodMisc"
names(food)[names(food) == "How many times did you dine out last week?"] <- "DiningOut"
cat("\n[Problem 2] Column names after renaming:\n")
print(names(food))
# ---- 2C. Helpers: numeric parsing & state normalization ----
# Parse numbers that may include $ and commas
parse_money <- function(x) {
if (is.numeric(x)) return(x)
x <- gsub("\\$", "", x)
x <- gsub(",", "", x)
suppressWarnings(as.numeric(x))
}
# Normalize State to USPS abbreviations; accept both full names and abbreviations
to_abb <- function(v) {
v_up <- toupper(trimws(as.character(v)))
# If already an abbreviation, keep; else try to map full name -> abb
# Build mapping
nm <- toupper(state.name)
ab <- state.abb
map <- setNames(ab, nm)
# Add DC mapping
map <- c(map, "DISTRICT OF COLUMBIA" = "DC", "WASHINGTON DC" = "DC", "WASHINGTON, DC" = "DC")
# If v_up in abbreviations, keep; else map full name
v_res <- ifelse(v_up %in% c(state.abb, "DC"), v_up,
ifelse(v_up %in% names(map), map[v_up], NA_character_))
v_res
}
# Coerce relevant columns
if (!is.numeric(food$Age))      food$Age      <- suppressWarnings(as.numeric(food$Age))
if (!is.numeric(food$DiningOut)) food$DiningOut <- suppressWarnings(as.numeric(food$DiningOut))
food$FoodTotal   <- parse_money(food$FoodTotal)
food$FoodGrocery <- parse_money(food$FoodGrocery)
food$FoodDining  <- parse_money(food$FoodDining)
food$FoodMisc    <- parse_money(food$FoodMisc)
food$StateAbb    <- to_abb(food$State)
food$CurrencyUP  <- toupper(trimws(as.character(food$Currency)))
# ---- 2D. Restrict to USD (show before/after) ----
n_before <- nrow(food)
food_usd <- subset(food, CurrencyUP == "USD")
n_after <- nrow(food_usd)
cat("\n[Problem 2] Restrict to USD — rows before/after:", n_before, "/", n_after, "\n")
# ---- 2E. Cleaning Rules (documented) ----
# Rules:
# - Age: keep 18–100 inclusive.
# - State: keep valid U.S. state abbreviations (50 states) + DC.
# - Expenditures: keep rows with all four expenditures in [0, 5000] per week (reasonable bounds).
# - Dining out: keep 0–50 per week (non-negative and plausible upper bound).
valid_states <- c(state.abb, "DC")
age_ok    <- with(food_usd, is.finite(Age) & Age >= 18 & Age <= 100)
state_ok  <- with(food_usd, !is.na(StateAbb) & StateAbb %in% valid_states)
exp_ok <- with(food_usd,
is.finite(FoodTotal)   & FoodTotal   >= 0 & FoodTotal   <= 5000 &
is.finite(FoodGrocery) & FoodGrocery >= 0 & FoodGrocery <= 5000 &
is.finite(FoodDining)  & FoodDining  >= 0 & FoodDining  <= 5000 &
is.finite(FoodMisc)    & FoodMisc    >= 0 & FoodMisc    <= 5000
)
dining_ok <- with(food_usd, is.finite(DiningOut) & DiningOut >= 0 & DiningOut <= 50)
food_clean <- food_usd[age_ok & state_ok & exp_ok & dining_ok, , drop = FALSE]
cat("\n[Problem 2] Cleaning summary:\n")
cat("- Age kept in [18,100]:", sum(age_ok), "rows passing\n")
cat("- State in 50 states + DC:", sum(state_ok), "rows passing\n")
cat("- All four expenditures in [0, 5000]:", sum(exp_ok), "rows passing\n")
cat("- Dining out in [0, 50]:", sum(dining_ok), "rows passing\n")
cat("\n[Problem 2] Final rows after all cleaning:", nrow(food_clean), "\n")
############################################################
# Problem 3 — Collatz Conjecture
############################################################
# ---- 3A. nextCollatz function ----
#' Compute the next number in the Collatz sequence.
#'
#' @param n Positive integer.
#' @return Integer: next Collatz value (n/2 if even; 3n+1 if odd).
#' @examples
#' nextCollatz(5)  # 16
#' nextCollatz(16) # 8
nextCollatz <- function(n) {
if (!is.numeric(n) || length(n) != 1 || !is.finite(n) || n <= 0 || n != as.integer(n)) {
stop("Input must be a single positive integer.")
}
n <- as.integer(n)
if (n %% 2 == 0L) n / 2L else 3L * n + 1L
}
# Demonstrations (will print the numbers)
cat("\n[Problem 3] nextCollatz(5):\n")
print(nextCollatz(5))
cat("[Problem 3] nextCollatz(16):\n")
print(nextCollatz(16))
# ---- 3B. collatzSequence function ----
#' Compute the full Collatz sequence and its length.
#'
#' @param n Positive integer.
#' @return A list with:
#'   - sequence: integer vector from n down to 1 via Collatz steps
#'   - length:   length of that vector
#' @examples
#' collatzSequence(5)$sequence
#' collatzSequence(19)$sequence
collatzSequence <- function(n) {
if (!is.numeric(n) || length(n) != 1 || !is.finite(n) || n <= 0 || n != as.integer(n)) {
stop("Input must be a single positive integer.")
}
n <- as.integer(n)
seqv <- n
while (n != 1L) {
n <- nextCollatz(n)
seqv <- c(seqv, n)
}
list(sequence = seqv, length = length(seqv))
}
# Demonstrations (will print the vectors)
cat("\n[Problem 3] collatzSequence(5)$sequence:\n")
print(collatzSequence(5)$sequence)
cat("[Problem 3] collatzSequence(19)$sequence:\n")
print(collatzSequence(19)$sequence)
# ---- 3C. Shortest & longest sequences for starts 100..500 (inclusive) ----
starts <- 100:500
lens <- vapply(starts, function(s) collatzSequence(s)$length, integer(1))
shortest_len <- min(lens)
longest_len  <- max(lens)
shortest_start <- starts[which(lens == shortest_len)][1]  # lowest start in case of tie
longest_start  <- starts[which(lens == longest_len)][1]   # lowest start in case of tie
cat("\n[Problem 3] Shortest Collatz sequence in 100..500:\n")
cat("- Start:", shortest_start, " Length:", shortest_len, "\n")
cat("\n[Problem 3] Longest Collatz sequence in 100..500:\n")
cat("- Start:", longest_start, " Length:", longest_len, "\n")
############################################################
# End of script
############################################################
library(tidyverse)
# ================================
# Setup
# ================================
# Install once if needed:
install.packages(c("tidyverse", "janitor", "rvest"))
library(janitor)
library(tidyverse)
library(janitor)
library(janitor)
library(janitor)
library(janitor)
library(janitor)
library(tidyverse)
library(janitor)
library(rvest)
set.seed(42)
# Helper to normalize column names safely
normalize_col <- function(df, candidates, target) {
hit <- intersect(names(df), candidates)
if (length(hit) == 1) df <- df %>% rename(!!target := all_of(hit))
df
}
# ============================================================
# Problem 1.1 — Long ↔ Wide using SOCR Housing Price dataset
# Link: https://wiki.socr.umich.edu/index.php/SOCR_Data_Dinov_010309_HousingPriceIndex
# ============================================================
# Scrape the main data table from the wiki page (robust even if CSV path changes)
housing_url <- "https://wiki.socr.umich.edu/index.php/SOCR_Data_Dinov_010309_HousingPriceIndex"
housing_page <- read_html(housing_url)
housing_long <- housing_page %>%
html_table(fill = TRUE) %>%
# pick the largest table (the main dataset)
(\(lst) lst[[which.max(sapply(lst, nrow))]])() %>%
as_tibble() %>%
clean_names()
# Normalize expected columns
housing_long <- housing_long %>%
normalize_col(c("state","state_name","states"), "state") %>%
normalize_col(c("year","yr"), "year") %>%
normalize_col(c("hpi","house_price_index"), "hpi") %>%
normalize_col(c("ur","unemployment_rate"), "ur") %>%
normalize_col(c("region","census_region"), "region") %>%
normalize_col(c("pop","population"), "pop") %>%
normalize_col(c("percent","pct","percent_subprime","percent_change"), "percent") %>%
mutate(year = suppressWarnings(as.integer(year)))
# Keep relevant variables if present
vars_needed <- c("state","year","hpi","ur","region","pop","percent")
housing_long <- housing_long %>% select(any_of(vars_needed))
# LONG → WIDE (one row per state, year-tagged columns)
housing_wide <- housing_long %>%
pivot_wider(
id_cols     = state,
names_from  = year,
values_from = c(hpi, ur, region, pop, percent),
names_glue  = "{.value}{year}"
)
# WIDE → LONG (recover original long/stacked format)
housing_long_back <- housing_wide %>%
pivot_longer(
cols = -state,
names_to = c(".value", "year"),
names_pattern = "([a-z_]+)(\\d+)"
) %>%
mutate(year = as.integer(year)) %>%
arrange(state, year)
# Quick integrity checks (only if both contain the same selected columns)
common_cols <- intersect(names(housing_long_back), names(housing_long))
stopifnot(nrow(housing_long_back %>% select(all_of(common_cols))) ==
nrow(housing_long %>% select(all_of(common_cols))))
# Peek results
cat("\n--- Problem 1.1: housing_wide preview ---\n"); print(head(housing_wide, 3))
cat("\n--- Problem 1.1: housing_long_back preview ---\n"); print(head(housing_long_back, 3))
# ======================================================================
# Problem 1.2 — Schizophrenia Neuroimaging Study (SOCR Oct 2009) tasks
# Link: https://wiki.socr.umich.edu/index.php/SOCR_Data_Oct2009_ID_NI#Data
# CSV:  https://wiki.socr.umich.edu/SOCR_Data/SOCR_Data_Oct2009_ID_NI.csv
# ======================================================================
schizo_url <- "https://wiki.socr.umich.edu/SOCR_Data/SOCR_Data_Oct2009_ID_NI.csv"
schizo <- readr::read_csv(schizo_url, show_col_types = FALSE) %>%
clean_names()
